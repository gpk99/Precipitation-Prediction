{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WZ8sqmSutqb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/data 2010-2020 (2).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vZzm5JFRu6mk"
      },
      "outputs": [],
      "source": [
        "df['time_bnds']=pd.to_datetime(df.time_bnds)\n",
        "df=df.sort_values(by='time_bnds').copy()\n",
        "df['Rainf_f_tavg']=df['Snowf_tavg']+df[\"Rainf_tavg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XfvgNjoeu8SC"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['rain'] = df['Rainf_f_tavg'].apply(lambda x: 0 if x == 0 else 1)\n",
        "df['snow'] = df['Snowf_tavg'].apply(lambda x: 0 if x == 0 else 1)\n",
        "df['only_rain'] = df['Rainf_tavg'].apply(lambda x: 0 if x == 0 else 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HETf4tx1u-Ch"
      },
      "outputs": [],
      "source": [
        "f_df=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqZUrRBzvAHN",
        "outputId": "c19fdb54-eacc-4ee0-8bfb-949fbf29e881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                time_bnds  Swnet_tavg_lag1  Lwnet_tavg_lag1  Qle_tavg_lag1  \\\n",
            "1     2010-01-01 03:00:00        10.520000        -6.457520       5.454976   \n",
            "2     2010-01-01 06:00:00         0.000000       -13.874785       8.162827   \n",
            "3     2010-01-01 09:00:00         0.000000        -5.900391      14.860957   \n",
            "4     2010-01-01 12:00:00         0.000000       -13.258985      18.593678   \n",
            "5     2010-01-01 15:00:00         0.000000       -13.336230      27.471237   \n",
            "...                   ...              ...              ...            ...   \n",
            "30425 2020-12-31 09:00:00         0.000000        -2.999004      17.126350   \n",
            "30426 2020-12-31 12:00:00         0.000000        -1.182715      12.212800   \n",
            "30427 2020-12-31 15:00:00         0.000000         0.184648       8.818521   \n",
            "30428 2020-12-31 18:00:00        37.560001        -0.453379      18.626129   \n",
            "30429 2020-12-31 21:00:00       142.929993        -7.088730      47.849495   \n",
            "\n",
            "       Qh_tavg_lag1  Qg_tavg_lag1  Snowf_tavg_lag1  Rainf_tavg_lag1  \\\n",
            "1         -5.315938      1.373789              0.0             0.00   \n",
            "2        -11.053730    -12.444258              0.0             0.00   \n",
            "3         -3.549609    -17.890528              0.0             0.00   \n",
            "4          2.510947    -34.360527              0.0             0.00   \n",
            "5         19.827909    -60.631016              0.0             0.00   \n",
            "...             ...           ...              ...              ...   \n",
            "30425      7.697187    -29.295860              0.0             0.00   \n",
            "30426     23.767929    -37.669395              0.0             0.00   \n",
            "30427     24.260626    -33.073742              0.0             0.00   \n",
            "30428     33.690235    -19.992247              0.0             7.77   \n",
            "30429     89.594162     -5.029131              0.0             3.11   \n",
            "\n",
            "       Evap_tavg_lag1  Qs_acc_lag1  ...  Rainf_f_tavg_lag5  Wind_f_inst_lag5  \\\n",
            "1            0.000002      0.00000  ...                NaN               NaN   \n",
            "2            0.000003      0.00000  ...                NaN               NaN   \n",
            "3            0.000006      0.00000  ...                NaN               NaN   \n",
            "4            0.000007      0.00000  ...                NaN               NaN   \n",
            "5            0.000011      0.00000  ...                0.0          4.207059   \n",
            "...               ...          ...  ...                ...               ...   \n",
            "30425        0.000007      0.00000  ...                0.1          3.003536   \n",
            "30426        0.000005      0.00000  ...                0.1          7.902500   \n",
            "30427        0.000003      0.00000  ...                0.0          4.207071   \n",
            "30428        0.000007      0.41101  ...                0.0          7.000000   \n",
            "30429        0.000019      0.06903  ...                0.0          5.804997   \n",
            "\n",
            "       Tair_f_inst_lag5  Qair_f_inst_lag5  Psurf_f_inst_lag5  \\\n",
            "1                   NaN               NaN                NaN   \n",
            "2                   NaN               NaN                NaN   \n",
            "3                   NaN               NaN                NaN   \n",
            "4                   NaN               NaN                NaN   \n",
            "5            276.145752          0.004552        99302.48438   \n",
            "...                 ...               ...                ...   \n",
            "30425        285.580658          0.007104        99096.22656   \n",
            "30426        286.582672          0.007737        98759.42188   \n",
            "30427        285.119751          0.008304        98934.06250   \n",
            "30428        279.457062          0.005654        99111.75000   \n",
            "30429        276.866242          0.004749        99329.49219   \n",
            "\n",
            "       SWdown_f_tavg_lag5  LWdown_f_tavg_lag5  rain_lag5  snow_lag5  \\\n",
            "1                     NaN                 NaN        NaN        NaN   \n",
            "2                     NaN                 NaN        NaN        NaN   \n",
            "3                     NaN                 NaN        NaN        NaN   \n",
            "4                     NaN                 NaN        NaN        NaN   \n",
            "5               13.390000          325.352661        0.0        0.0   \n",
            "...                   ...                 ...        ...        ...   \n",
            "30425          208.990005          341.379852        1.0        0.0   \n",
            "30426          139.330002          362.720032        1.0        0.0   \n",
            "30427           16.150000          369.327911        0.0        0.0   \n",
            "30428            0.000000          363.486847        0.0        0.0   \n",
            "30429            0.000000          336.563904        0.0        0.0   \n",
            "\n",
            "       only_rain_lag5  \n",
            "1                 NaN  \n",
            "2                 NaN  \n",
            "3                 NaN  \n",
            "4                 NaN  \n",
            "5                 0.0  \n",
            "...               ...  \n",
            "30425             1.0  \n",
            "30426             1.0  \n",
            "30427             0.0  \n",
            "30428             0.0  \n",
            "30429             0.0  \n",
            "\n",
            "[32143 rows x 196 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming f_df is your DataFrame\n",
        "lag_range = range(1, 6)  # Lag range from 1 to 24\n",
        "\n",
        "# Create a list to store lagged DataFrames\n",
        "lagged_dfs = []\n",
        "\n",
        "# Iterate through each lag and create lagged DataFrames\n",
        "for lag in lag_range:\n",
        "    lagged_df = f_df.copy()\n",
        "    lagged_df.iloc[:, 1:] = f_df.iloc[:, 1:].shift(periods=lag)\n",
        "    lagged_df.columns = [col + f'_lag{lag}' if col != 'time_bnds' else col for col in lagged_df.columns]\n",
        "    if lag>1:\n",
        "      lagged_df=lagged_df.iloc[:,1:]\n",
        "    lagged_dfs.append(lagged_df.iloc[lag:, :])  # Drop rows with NaN values due to shifting\n",
        "\n",
        "# Concatenate all lagged DataFrames\n",
        "final_df = pd.concat(lagged_dfs, axis=1)\n",
        "\n",
        "print(final_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u_OzrN_mvB6y"
      },
      "outputs": [],
      "source": [
        "# no lag variables\n",
        "target=df[[\"time_bnds\",\"Rainf_f_tavg\",\"Snowf_tavg\",\"Rainf_tavg\",'rain',\"only_rain\",\"snow\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xew8MfwfvFxy"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(final_df, target, on='time_bnds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mOVQgL2bvH6V"
      },
      "outputs": [],
      "source": [
        "merged_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mQYqZXAkvL0j"
      },
      "outputs": [],
      "source": [
        "def remove_lag_substring(string_list):\n",
        "    modified_strings = []\n",
        "    for string in string_list:\n",
        "        if \"_lag\" in string:\n",
        "            modified_string = string.split(\"_lag\")[0]  # Split at \"_lag\" and take the first part\n",
        "            modified_strings.append(modified_string)\n",
        "        else:\n",
        "            modified_strings.append(string)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_modified_strings = list(set(modified_strings))\n",
        "\n",
        "    return unique_modified_strings\n",
        "\n",
        "# Example list of strings\n",
        "\n",
        "\n",
        "all_featurs=remove_lag_substring(merged_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "njsB0CMAvYXH"
      },
      "outputs": [],
      "source": [
        "merged_df['month'] = merged_df['time_bnds'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5GIpWCrvaxN",
        "outputId": "0b5641f1-6eb1-4602-acd5-b3a4d7394040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                time_bnds  Swnet_tavg_lag1  Lwnet_tavg_lag1  Qle_tavg_lag1  \\\n",
            "4     2010-01-01 15:00:00         0.000000       -13.336230      27.471237   \n",
            "5     2010-01-01 18:00:00        63.099998       -71.594139      35.745373   \n",
            "6     2010-01-01 21:00:00       329.089996       -99.904099      89.040543   \n",
            "7     2010-01-02 00:00:00       297.399994      -101.335724      89.145309   \n",
            "8     2010-01-02 03:00:00        32.720001       -82.362770      31.837286   \n",
            "...                   ...              ...              ...            ...   \n",
            "32138 2020-12-31 09:00:00         0.000000        -2.999004      17.126350   \n",
            "32139 2020-12-31 12:00:00         0.000000        -1.182715      12.212800   \n",
            "32140 2020-12-31 15:00:00         0.000000         0.184648       8.818521   \n",
            "32141 2020-12-31 18:00:00        37.560001        -0.453379      18.626129   \n",
            "32142 2020-12-31 21:00:00       142.929993        -7.088730      47.849495   \n",
            "\n",
            "       Qh_tavg_lag1  Qg_tavg_lag1  Snowf_tavg_lag1  Rainf_tavg_lag1  \\\n",
            "4         19.827909    -60.631016              0.0             0.00   \n",
            "5         28.958574    -73.185974              0.0             0.00   \n",
            "6        163.722229    -23.466534              0.0             0.00   \n",
            "7        119.349724    -12.572696              0.0             0.00   \n",
            "8        -23.531231    -57.941601              0.0             0.00   \n",
            "...             ...           ...              ...              ...   \n",
            "32138      7.697187    -29.295860              0.0             0.00   \n",
            "32139     23.767929    -37.669395              0.0             0.00   \n",
            "32140     24.260626    -33.073742              0.0             0.00   \n",
            "32141     33.690235    -19.992247              0.0             7.77   \n",
            "32142     89.594162     -5.029131              0.0             3.11   \n",
            "\n",
            "       Evap_tavg_lag1  Qs_acc_lag1  ...  snow_lag5  only_rain_lag5  \\\n",
            "4            0.000011      0.00000  ...        0.0             0.0   \n",
            "5            0.000014      0.00000  ...        0.0             0.0   \n",
            "6            0.000035      0.00000  ...        0.0             0.0   \n",
            "7            0.000035      0.00000  ...        0.0             0.0   \n",
            "8            0.000012      0.00000  ...        0.0             0.0   \n",
            "...               ...          ...  ...        ...             ...   \n",
            "32138        0.000007      0.00000  ...        0.0             1.0   \n",
            "32139        0.000005      0.00000  ...        0.0             1.0   \n",
            "32140        0.000003      0.00000  ...        0.0             0.0   \n",
            "32141        0.000007      0.41101  ...        0.0             0.0   \n",
            "32142        0.000019      0.06903  ...        0.0             0.0   \n",
            "\n",
            "       Rainf_f_tavg  Snowf_tavg  Rainf_tavg  rain  only_rain  snow  month  \\\n",
            "4              0.00         0.0        0.00     0          0     0      1   \n",
            "5              0.00         0.0        0.00     0          0     0      1   \n",
            "6              0.00         0.0        0.00     0          0     0      1   \n",
            "7              0.00         0.0        0.00     0          0     0      1   \n",
            "8              0.00         0.0        0.00     0          0     0      1   \n",
            "...             ...         ...         ...   ...        ...   ...    ...   \n",
            "32138          0.00         0.0        0.00     0          0     0     12   \n",
            "32139          0.00         0.0        0.00     0          0     0     12   \n",
            "32140          7.77         0.0        7.77     1          1     0     12   \n",
            "32141          3.11         0.0        3.11     1          1     0     12   \n",
            "32142          0.00         0.0        0.00     0          0     0     12   \n",
            "\n",
            "       Rain_classification  \n",
            "4                        0  \n",
            "5                        0  \n",
            "6                        0  \n",
            "7                        0  \n",
            "8                        0  \n",
            "...                    ...  \n",
            "32138                    0  \n",
            "32139                    0  \n",
            "32140                    2  \n",
            "32141                    2  \n",
            "32142                    0  \n",
            "\n",
            "[32139 rows x 204 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a function to apply the conditions\n",
        "def classify_rain(value):\n",
        "    if value == 0:\n",
        "        return 0\n",
        "    elif 0 < value < 3:\n",
        "        return 1\n",
        "    elif 3 <= value < 10:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "\n",
        "# Assuming df is your DataFrame and 'rainf_f_tavg' is the column containing rain data\n",
        "merged_df['Rain_classification'] = merged_df['Rainf_f_tavg'].apply(classify_rain)\n",
        "\n",
        "# Print the DataFrame to verify the new column\n",
        "print(merged_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRsIE0EZUYwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzxgPuCFwKnN",
        "outputId": "6904f715-aee7-438d-fef0-4353bb95145f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 23336\n",
            "Class 1: 7511\n",
            "Class 2: 1083\n",
            "Class 3: 209\n"
          ]
        }
      ],
      "source": [
        "# Assuming df is your DataFrame containing the 'rain_classification' column\n",
        "class_sizes = merged_df['Rain_classification'].value_counts()\n",
        "\n",
        "# Print the size of each class\n",
        "for class_label, size in class_sizes.items():\n",
        "    print(f\"Class {class_label}: {size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9J1h3dKwNzq"
      },
      "outputs": [],
      "source": [
        "def remove_lag_substring(string_list):\n",
        "    modified_strings = []\n",
        "    for string in string_list:\n",
        "        if \"_lag\" in string:\n",
        "            modified_string = string.split(\"_lag\")[0]  # Split at \"_lag\" and take the first part\n",
        "            modified_strings.append(modified_string)\n",
        "        else:\n",
        "            modified_strings.append(string)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_modified_strings = list(set(modified_strings))\n",
        "\n",
        "    return unique_modified_strings\n",
        "\n",
        "# Example list of strings\n",
        "\n",
        "\n",
        "all_featurs=remove_lag_substring(merged_df.columns)\n",
        "\n",
        "def fetch_items_with_start_and_end_substrings(my_list, start_substrings, end_substrings):\n",
        "    matching_items = [item for item in my_list if any(item.startswith(start_sub) for start_sub in start_substrings)\n",
        "                                                 and any(item.endswith(end_sub) for end_sub in end_substrings)]\n",
        "    return matching_items\n",
        "\n",
        "lag=[\"lag1\"]\n",
        "additional=[]\n",
        "exclude=['snow','only_rain','rain','Snowf_tavg','Rainf_tavg']\n",
        "#exclude=['Snowf_tavg','Rainf_tavg','Rainf_f_tavg','SoilMoi10_40cm_inst','SoilMoi40_100cm_inst','SoilMoi100_200cm_inst','SoilTMP100_200cm_inst','SoilTMP10_40cm_inst','SoilTMP40_100cm_inst','snow','only_rain']\n",
        "\n",
        "# Remove items from featurs that are in exclude\n",
        "all_featurs = [item for item in all_featurs if item not in exclude]\n",
        "featurs=all_featurs[:]\n",
        "variables =fetch_items_with_start_and_end_substrings(merged_df.columns, featurs, lag)\n",
        "variables.extend(additional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doLj6ZoOwcW1"
      },
      "outputs": [],
      "source": [
        "variables=['Rainf_tavg_lag1',\"Qs_acc_lag1\",'LWdown_f_tavg_lag1',\"Qair_f_inst_lag1\",\"ECanop_tavg_lag1\",\"AvgSurfT_inst_lag1\",\"SnowDepth_inst_lag1\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "months_to_include = [3,4,5,6,7,8]\n",
        "merged_df = merged_df[merged_df['month'].isin(months_to_include)]"
      ],
      "metadata": {
        "id": "7DpsoTKGDDbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M84_rzJUDVKT",
        "outputId": "dd6b8a8a-b693-4e1e-dc66-d9f61fbbdd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Swnet_tavg_lag1',\n",
              " 'Lwnet_tavg_lag1',\n",
              " 'Qle_tavg_lag1',\n",
              " 'Qh_tavg_lag1',\n",
              " 'Qg_tavg_lag1',\n",
              " 'Evap_tavg_lag1',\n",
              " 'Qs_acc_lag1',\n",
              " 'Qsb_acc_lag1',\n",
              " 'Qsm_acc_lag1',\n",
              " 'AvgSurfT_inst_lag1',\n",
              " 'Albedo_inst_lag1',\n",
              " 'SWE_inst_lag1',\n",
              " 'SnowDepth_inst_lag1',\n",
              " 'SoilMoi0_10cm_inst_lag1',\n",
              " 'SoilMoi10_40cm_inst_lag1',\n",
              " 'SoilMoi40_100cm_inst_lag1',\n",
              " 'SoilMoi100_200cm_inst_lag1',\n",
              " 'SoilTMP0_10cm_inst_lag1',\n",
              " 'SoilTMP10_40cm_inst_lag1',\n",
              " 'SoilTMP40_100cm_inst_lag1',\n",
              " 'SoilTMP100_200cm_inst_lag1',\n",
              " 'PotEvap_tavg_lag1',\n",
              " 'ECanop_tavg_lag1',\n",
              " 'Tveg_tavg_lag1',\n",
              " 'ESoil_tavg_lag1',\n",
              " 'RootMoist_inst_lag1',\n",
              " 'CanopInt_inst_lag1',\n",
              " 'Rainf_f_tavg_lag1',\n",
              " 'Wind_f_inst_lag1',\n",
              " 'Tair_f_inst_lag1',\n",
              " 'Qair_f_inst_lag1',\n",
              " 'Psurf_f_inst_lag1',\n",
              " 'SWdown_f_tavg_lag1',\n",
              " 'LWdown_f_tavg_lag1']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbVW5Z0qwfCY",
        "outputId": "83a3a4d8-6648-44d4-b4ed-7baf4dd79980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      8661\n",
            "           1       0.78      0.83      0.80      3741\n",
            "           2       0.78      0.36      0.50       464\n",
            "           3       0.79      0.13      0.22        87\n",
            "\n",
            "    accuracy                           0.87     12953\n",
            "   macro avg       0.81      0.56      0.61     12953\n",
            "weighted avg       0.87      0.87      0.87     12953\n",
            "\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91      2208\n",
            "           1       0.72      0.77      0.74       892\n",
            "           2       0.48      0.23      0.31       122\n",
            "           3       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.84      3239\n",
            "   macro avg       0.53      0.48      0.49      3239\n",
            "weighted avg       0.83      0.84      0.84      3239\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "               Feature  Importance\n",
            "0      Rainf_tavg_lag1    0.508868\n",
            "1          Qs_acc_lag1    0.215175\n",
            "4     ECanop_tavg_lag1    0.101480\n",
            "2   LWdown_f_tavg_lag1    0.066719\n",
            "3     Qair_f_inst_lag1    0.058283\n",
            "5   AvgSurfT_inst_lag1    0.041420\n",
            "6  SnowDepth_inst_lag1    0.008053\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'date' column exists in merged_df DataFrame\n",
        "merged_df['time_bnds'] = pd.to_datetime(merged_df['time_bnds'])\n",
        "\n",
        "\n",
        "\n",
        "# Assuming df is your DataFrame and 'rainf_f_tavg' is the target variable\n",
        "X = merged_df[variables]  # Features\n",
        "y = merged_df['Rain_classification']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate the Random Forest classifier\n",
        "model = RandomForestClassifier(n_estimators=10, max_depth=10, min_samples_split=5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Print classification reports\n",
        "print(\"Train Classification Report:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Print feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "sorted_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(sorted_importance_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDmoHNPFw31s",
        "outputId": "b01d62e3-5e6e-4ae2-a483-ce90fa3b5f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91      8661\n",
            "           1       0.75      0.79      0.77      3741\n",
            "           2       0.42      0.32      0.36       464\n",
            "           3       0.00      0.00      0.00        87\n",
            "\n",
            "    accuracy                           0.85     12953\n",
            "   macro avg       0.52      0.50      0.51     12953\n",
            "weighted avg       0.84      0.85      0.84     12953\n",
            "\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91      2208\n",
            "           1       0.74      0.76      0.75       892\n",
            "           2       0.44      0.32      0.37       122\n",
            "           3       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.85      3239\n",
            "   macro avg       0.52      0.50      0.51      3239\n",
            "weighted avg       0.84      0.85      0.84      3239\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame and 'rainf_f_tavg' is the target variable\n",
        "X = merged_df[variables]  # Features\n",
        "y = merged_df['Rain_classification']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate the XGBoost classifier\n",
        "model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.001, max_depth=2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training and testing sets\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Train classification report\n",
        "print(\"Train Classification Report:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "# Test classification report\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train"
      ],
      "metadata": {
        "id": "ZUGxDcRY-1TM",
        "outputId": "868ee4d8-2caa-49e6-a86c-993528c8d180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFnoCtTH-DQl",
        "outputId": "8a9dc053-a2ee-4feb-99cb-314a0cb21f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91      8661\n",
            "           1       0.74      0.80      0.77      3741\n",
            "           2       0.47      0.23      0.31       464\n",
            "           3       0.80      0.05      0.09        87\n",
            "\n",
            "    accuracy                           0.85     12953\n",
            "   macro avg       0.73      0.50      0.52     12953\n",
            "weighted avg       0.84      0.85      0.84     12953\n",
            "\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91      2208\n",
            "           1       0.73      0.78      0.75       892\n",
            "           2       0.47      0.23      0.31       122\n",
            "           3       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.85      3239\n",
            "   macro avg       0.53      0.48      0.49      3239\n",
            "weighted avg       0.84      0.85      0.84      3239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame and 'rainf_f_tavg' is the target variable\n",
        "X = merged_df[variables]  # Features\n",
        "y = merged_df['Rain_classification']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)  # Use the same scaler fitted on training data for test data\n",
        "\n",
        "# Instantiate the XGBoost classifier\n",
        "model = xgb.XGBClassifier(n_estimators=200, learning_rate=0.001, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model on scaled data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the training and testing sets\n",
        "y_pred_train = model.predict(X_train_scaled)\n",
        "y_pred_test = model.predict(X_test_scaled)\n",
        "\n",
        "# Train classification report\n",
        "print(\"Train Classification Report:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "# Test classification report\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9tM3SiVzjAU",
        "outputId": "27ea421f-a03e-4d75-ee1a-80638ddb32cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Predictions DataFrame:\n",
            "       prediction  actual  month\n",
            "14052           0       0     10\n",
            "3141            0       0      1\n",
            "9020            0       0      2\n",
            "19927           1       0     10\n",
            "5530            1       1     11\n",
            "\n",
            "Testing Predictions DataFrame:\n",
            "       prediction  actual  month\n",
            "12025           0       0      2\n",
            "29552           0       0      2\n",
            "3135            0       0      1\n",
            "8940            0       0      1\n",
            "19999           0       0     11\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the training and testing sets\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Create DataFrame for training predictions\n",
        "train_predictions_df = pd.DataFrame({\n",
        "    'prediction': y_pred_train,\n",
        "    'actual': y_train,\n",
        "    'month': X_train['month']  # Assuming 'month' is a column in X_train\n",
        "})\n",
        "\n",
        "# Create DataFrame for testing predictions\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'prediction': y_pred_test,\n",
        "    'actual': y_test,\n",
        "    'month': X_test['month']  # Assuming 'month' is a column in X_test\n",
        "})\n",
        "\n",
        "# Display the first few rows of the training predictions DataFrame\n",
        "print(\"Training Predictions DataFrame:\")\n",
        "print(train_predictions_df.head())\n",
        "\n",
        "# Display the first few rows of the testing predictions DataFrame\n",
        "print(\"\\nTesting Predictions DataFrame:\")\n",
        "print(test_predictions_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maTd1eHQ-lYY",
        "outputId": "c0ba997d-5af8-415e-eb0d-ce4eb102c242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'unique'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b86875b5ad0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvj8vvMbzpu3",
        "outputId": "592333cb-8d70-40e9-f85d-92ba30ba615d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Month 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9053    0.9607    0.9322       458\n",
            "           1     0.7595    0.6061    0.6742        99\n",
            "           2     0.4615    0.3529    0.4000        17\n",
            "           3     0.0000    0.0000    0.0000         4\n",
            "\n",
            "    accuracy                         0.8754       578\n",
            "   macro avg     0.5316    0.4799    0.5016       578\n",
            "weighted avg     0.8610    0.8754    0.8659       578\n",
            "\n",
            "\n",
            "\n",
            "Classification Report for Month 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8980    0.9312    0.9143       378\n",
            "           1     0.6712    0.5506    0.6049        89\n",
            "           2     0.4000    0.5333    0.4571        15\n",
            "           3     0.0000    0.0000    0.0000         3\n",
            "\n",
            "    accuracy                         0.8433       485\n",
            "   macro avg     0.4923    0.5038    0.4941       485\n",
            "weighted avg     0.8354    0.8433    0.8377       485\n",
            "\n",
            "\n",
            "\n",
            "Classification Report for Month 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9463    0.9391    0.9427       394\n",
            "           1     0.7273    0.7767    0.7512       103\n",
            "           2     0.4000    0.3333    0.3636        18\n",
            "           3     0.0000    0.0000    0.0000         1\n",
            "\n",
            "    accuracy                         0.8837       516\n",
            "   macro avg     0.5184    0.5123    0.5144       516\n",
            "weighted avg     0.8817    0.8837    0.8824       516\n",
            "\n",
            "\n",
            "\n",
            "Classification Report for Month 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9378    0.9333    0.9356       420\n",
            "           1     0.6857    0.7129    0.6990       101\n",
            "           2     0.3333    0.3500    0.3415        20\n",
            "           3     0.0000    0.0000    0.0000         3\n",
            "\n",
            "    accuracy                         0.8658       544\n",
            "   macro avg     0.4892    0.4991    0.4940       544\n",
            "weighted avg     0.8636    0.8658    0.8646       544\n",
            "\n",
            "\n",
            "\n",
            "Classification Report for Month 11:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9402    0.9381    0.9392       436\n",
            "           1     0.7245    0.7717    0.7474        92\n",
            "           2     0.7143    0.5556    0.6250        18\n",
            "           3     1.0000    0.5000    0.6667         2\n",
            "\n",
            "    accuracy                         0.8960       548\n",
            "   macro avg     0.8448    0.6913    0.7445       548\n",
            "weighted avg     0.8968    0.8960    0.8956       548\n",
            "\n",
            "\n",
            "\n",
            "Classification Report for Month 12:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9372    0.9419    0.9395       396\n",
            "           1     0.7447    0.7292    0.7368        96\n",
            "           2     0.6667    0.6667    0.6667        24\n",
            "           3     0.0000    0.0000    0.0000         3\n",
            "\n",
            "    accuracy                         0.8844       519\n",
            "   macro avg     0.5871    0.5844    0.5858       519\n",
            "weighted avg     0.8837    0.8844    0.8840       519\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming prediction_actual_month is your DataFrame\n",
        "# Iterate over unique months in the DataFrame\n",
        "unique_months = [1,2,9,10,11,12]\n",
        "\n",
        "for month in unique_months:\n",
        "    # Filter data for the current month\n",
        "    month_data = test_predictions_df[test_predictions_df['month'] == month]\n",
        "\n",
        "    # Extract actual and predicted labels for the current month\n",
        "    y_actual = month_data['actual']\n",
        "    y_pred = month_data['prediction']\n",
        "\n",
        "    # Generate classification report for the current month\n",
        "    report = classification_report(y_actual, y_pred, digits=4)  # You can adjust digits for precision\n",
        "\n",
        "    # Print the classification report for the current month\n",
        "    print(f\"Classification Report for Month {month}:\")\n",
        "    print(report)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the classification report to extract class-specific metrics\n",
        "lines = report.split('\\n')\n",
        "class_2_metrics = lines[3].split()  # Assuming class 2 is the fourth line (index 3) in the report\n",
        "class_2_recall = float(class_2_metrics[3])  # Recall for class 2 is the fourth element in the line\n",
        "\n",
        "print(f\"Recall for Class 2: {class_2_recall:.4f}\")"
      ],
      "metadata": {
        "id": "1wM0ForP_Cee",
        "outputId": "fcae5810-fd12-402a-df4d-7fd7fe8e75cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall for Class 2: 0.6980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x9880R3H9zoh",
        "outputId": "28ced8e3-2bc9-46fb-ae0b-836af4209696"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93     18639\n",
            "           1       0.78      0.72      0.75      6066\n",
            "           2       0.54      0.37      0.44       838\n",
            "           3       0.50      0.12      0.20       168\n",
            "\n",
            "    accuracy                           0.87     25711\n",
            "   macro avg       0.68      0.54      0.58     25711\n",
            "weighted avg       0.86      0.87      0.86     25711\n",
            "\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92      4697\n",
            "           1       0.71      0.66      0.68      1445\n",
            "           2       0.41      0.29      0.34       245\n",
            "           3       0.33      0.05      0.09        41\n",
            "\n",
            "    accuracy                           0.84      6428\n",
            "   macro avg       0.59      0.48      0.51      6428\n",
            "weighted avg       0.83      0.84      0.84      6428\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Instantiate the MLPClassifier\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(200,100, 50), activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Train the MLP model on the scaled training data\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the training and testing sets\n",
        "y_pred_train = mlp_model.predict(X_train_scaled)\n",
        "y_pred_test = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# Print classification reports for training and testing predictions\n",
        "print(\"Train Classification Report:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU8MObmr_Q6m",
        "outputId": "f82ef1f1-3be7-4b8b-a760-e8f706e5db8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.91     18639\n",
            "           1       0.69      0.57      0.62      6066\n",
            "           2       0.50      0.07      0.13       838\n",
            "           3       0.30      0.04      0.07       168\n",
            "\n",
            "    accuracy                           0.83     25711\n",
            "   macro avg       0.59      0.41      0.43     25711\n",
            "weighted avg       0.81      0.83      0.81     25711\n",
            "\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91      4697\n",
            "           1       0.66      0.57      0.62      1445\n",
            "           2       0.45      0.09      0.16       245\n",
            "           3       0.45      0.12      0.19        41\n",
            "\n",
            "    accuracy                           0.83      6428\n",
            "   macro avg       0.61      0.43      0.47      6428\n",
            "weighted avg       0.81      0.83      0.81      6428\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Instantiate the MLPClassifier with modified architecture\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(200, 100, 50),  # Increase layer sizes\n",
        "    activation='relu',                  # Use ReLU activation\n",
        "    solver='adam',                      # Use Adam optimizer\n",
        "    alpha=0.0001,                        # L2 penalty (regularization)\n",
        "    batch_size=64,                      # Number of samples per batch           # Adaptive learning rate                      # Increase max iterations\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the MLP model on the scaled training data\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the training and testing sets\n",
        "y_pred_train = mlp_model.predict(X_train_scaled)\n",
        "y_pred_test = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# Print classification reports for training and testing predictions\n",
        "print(\"Train Classification Report:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variables=['Swnet_tavg_lag1',\n",
        " 'Lwnet_tavg_lag1',\n",
        " 'Qle_tavg_lag1',\n",
        " 'Qh_tavg_lag1',\n",
        " 'Qg_tavg_lag1',\n",
        " 'Evap_tavg_lag1',\n",
        " 'Qs_acc_lag1',\n",
        " 'Qsb_acc_lag1',\n",
        " 'Qsm_acc_lag1',\n",
        " 'AvgSurfT_inst_lag1',\n",
        " 'Albedo_inst_lag1',\n",
        " 'SWE_inst_lag1',\n",
        " 'SnowDepth_inst_lag1',\n",
        " 'SoilMoi0_10cm_inst_lag1',\n",
        " 'SoilTMP0_10cm_inst_lag1',\n",
        " 'PotEvap_tavg_lag1',\n",
        " 'ECanop_tavg_lag1',\n",
        " 'Tveg_tavg_lag1',\n",
        " 'ESoil_tavg_lag1',\n",
        " 'RootMoist_inst_lag1',\n",
        " 'CanopInt_inst_lag1',\n",
        " 'Rainf_f_tavg_lag1',\n",
        " 'Wind_f_inst_lag1',\n",
        " 'Tair_f_inst_lag1',\n",
        " 'Qair_f_inst_lag1',\n",
        " 'Psurf_f_inst_lag1',\n",
        " 'SWdown_f_tavg_lag1',\n",
        " 'LWdown_f_tavg_lag1']"
      ],
      "metadata": {
        "id": "_BIad1kIrEbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame and 'Rain_classification' is the target variable\n",
        "X = merged_df[variables]  # Features\n",
        "y = merged_df['Rain_classification']  # Target variable (categorical)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)  # Use the same scaler fitted on training data for test data\n",
        "\n",
        "# Convert target variable to categorical format\n",
        "y_train_categorical = pd.get_dummies(y_train)  # Convert training labels to one-hot encoding\n",
        "y_test_categorical = pd.get_dummies(y_test)    # Convert test labels to one-hot encoding\n",
        "\n",
        "# Define the FNN model for multiclass classification\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),  # Additional dense layer\n",
        "    tf.keras.layers.Dense(len(y_train_categorical.columns), activation='softmax')\n",
        "    # Output layer with softmax activation for multiclass classification\n",
        "])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_categorical, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model on training set\n",
        "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train_categorical, verbose=0)\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Test classification report\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R4pWAm0fG8g",
        "outputId": "24dbc2b5-68dc-4400-fe35-2acf029e31c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1286/1286 [==============================] - 7s 4ms/step - loss: 0.6505 - accuracy: 0.7571 - val_loss: 0.5233 - val_accuracy: 0.7945\n",
            "Epoch 2/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.5132 - accuracy: 0.8040 - val_loss: 0.5071 - val_accuracy: 0.8050\n",
            "Epoch 3/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.4998 - accuracy: 0.8100 - val_loss: 0.4994 - val_accuracy: 0.8059\n",
            "Epoch 4/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4899 - accuracy: 0.8131 - val_loss: 0.4957 - val_accuracy: 0.8085\n",
            "Epoch 5/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4831 - accuracy: 0.8154 - val_loss: 0.4884 - val_accuracy: 0.8120\n",
            "Epoch 6/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4770 - accuracy: 0.8165 - val_loss: 0.4854 - val_accuracy: 0.8151\n",
            "Epoch 7/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4718 - accuracy: 0.8192 - val_loss: 0.4802 - val_accuracy: 0.8161\n",
            "Epoch 8/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4669 - accuracy: 0.8217 - val_loss: 0.4791 - val_accuracy: 0.8201\n",
            "Epoch 9/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4625 - accuracy: 0.8240 - val_loss: 0.4823 - val_accuracy: 0.8168\n",
            "Epoch 10/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4585 - accuracy: 0.8254 - val_loss: 0.4757 - val_accuracy: 0.8221\n",
            "Epoch 11/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4540 - accuracy: 0.8270 - val_loss: 0.4739 - val_accuracy: 0.8229\n",
            "Epoch 12/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.4514 - accuracy: 0.8275 - val_loss: 0.4715 - val_accuracy: 0.8194\n",
            "Epoch 13/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.4475 - accuracy: 0.8305 - val_loss: 0.4715 - val_accuracy: 0.8215\n",
            "Epoch 14/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4441 - accuracy: 0.8307 - val_loss: 0.4659 - val_accuracy: 0.8254\n",
            "Epoch 15/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4404 - accuracy: 0.8323 - val_loss: 0.4655 - val_accuracy: 0.8285\n",
            "Epoch 16/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4379 - accuracy: 0.8336 - val_loss: 0.4678 - val_accuracy: 0.8258\n",
            "Epoch 17/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4350 - accuracy: 0.8349 - val_loss: 0.4662 - val_accuracy: 0.8248\n",
            "Epoch 18/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4330 - accuracy: 0.8354 - val_loss: 0.4591 - val_accuracy: 0.8277\n",
            "Epoch 19/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4281 - accuracy: 0.8384 - val_loss: 0.4808 - val_accuracy: 0.8256\n",
            "Epoch 20/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.4266 - accuracy: 0.8392 - val_loss: 0.4617 - val_accuracy: 0.8304\n",
            "Epoch 21/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4234 - accuracy: 0.8406 - val_loss: 0.4616 - val_accuracy: 0.8277\n",
            "Epoch 22/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.4207 - accuracy: 0.8408 - val_loss: 0.4594 - val_accuracy: 0.8287\n",
            "Epoch 23/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4179 - accuracy: 0.8428 - val_loss: 0.4552 - val_accuracy: 0.8303\n",
            "Epoch 24/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4151 - accuracy: 0.8427 - val_loss: 0.4562 - val_accuracy: 0.8304\n",
            "Epoch 25/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4135 - accuracy: 0.8451 - val_loss: 0.4551 - val_accuracy: 0.8295\n",
            "Epoch 26/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4116 - accuracy: 0.8449 - val_loss: 0.4516 - val_accuracy: 0.8324\n",
            "Epoch 27/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.4080 - accuracy: 0.8468 - val_loss: 0.4589 - val_accuracy: 0.8299\n",
            "Epoch 28/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4069 - accuracy: 0.8454 - val_loss: 0.4532 - val_accuracy: 0.8283\n",
            "Epoch 29/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4047 - accuracy: 0.8490 - val_loss: 0.4573 - val_accuracy: 0.8304\n",
            "Epoch 30/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4028 - accuracy: 0.8464 - val_loss: 0.4516 - val_accuracy: 0.8318\n",
            "Epoch 31/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.4011 - accuracy: 0.8495 - val_loss: 0.4506 - val_accuracy: 0.8314\n",
            "Epoch 32/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3985 - accuracy: 0.8506 - val_loss: 0.4525 - val_accuracy: 0.8281\n",
            "Epoch 33/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3974 - accuracy: 0.8510 - val_loss: 0.4584 - val_accuracy: 0.8285\n",
            "Epoch 34/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3955 - accuracy: 0.8517 - val_loss: 0.4536 - val_accuracy: 0.8320\n",
            "Epoch 35/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3936 - accuracy: 0.8534 - val_loss: 0.4496 - val_accuracy: 0.8312\n",
            "Epoch 36/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3915 - accuracy: 0.8536 - val_loss: 0.4583 - val_accuracy: 0.8320\n",
            "Epoch 37/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3894 - accuracy: 0.8544 - val_loss: 0.4592 - val_accuracy: 0.8291\n",
            "Epoch 38/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3880 - accuracy: 0.8545 - val_loss: 0.4628 - val_accuracy: 0.8345\n",
            "Epoch 39/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3867 - accuracy: 0.8539 - val_loss: 0.4544 - val_accuracy: 0.8297\n",
            "Epoch 40/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3848 - accuracy: 0.8557 - val_loss: 0.4578 - val_accuracy: 0.8291\n",
            "Epoch 41/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3823 - accuracy: 0.8568 - val_loss: 0.4519 - val_accuracy: 0.8326\n",
            "Epoch 42/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3819 - accuracy: 0.8579 - val_loss: 0.4615 - val_accuracy: 0.8314\n",
            "Epoch 43/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3800 - accuracy: 0.8587 - val_loss: 0.4550 - val_accuracy: 0.8297\n",
            "Epoch 44/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3792 - accuracy: 0.8603 - val_loss: 0.4586 - val_accuracy: 0.8338\n",
            "Epoch 45/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3760 - accuracy: 0.8588 - val_loss: 0.4579 - val_accuracy: 0.8279\n",
            "Epoch 46/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3764 - accuracy: 0.8588 - val_loss: 0.4547 - val_accuracy: 0.8336\n",
            "Epoch 47/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3749 - accuracy: 0.8586 - val_loss: 0.4618 - val_accuracy: 0.8260\n",
            "Epoch 48/100\n",
            "1286/1286 [==============================] - 3s 3ms/step - loss: 0.3731 - accuracy: 0.8597 - val_loss: 0.4570 - val_accuracy: 0.8289\n",
            "Epoch 49/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3714 - accuracy: 0.8620 - val_loss: 0.4551 - val_accuracy: 0.8341\n",
            "Epoch 50/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3703 - accuracy: 0.8609 - val_loss: 0.4587 - val_accuracy: 0.8312\n",
            "Epoch 51/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3672 - accuracy: 0.8624 - val_loss: 0.4774 - val_accuracy: 0.8303\n",
            "Epoch 52/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3676 - accuracy: 0.8619 - val_loss: 0.4605 - val_accuracy: 0.8320\n",
            "Epoch 53/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3650 - accuracy: 0.8640 - val_loss: 0.4661 - val_accuracy: 0.8234\n",
            "Epoch 54/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3640 - accuracy: 0.8629 - val_loss: 0.4616 - val_accuracy: 0.8339\n",
            "Epoch 55/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3627 - accuracy: 0.8653 - val_loss: 0.4595 - val_accuracy: 0.8268\n",
            "Epoch 56/100\n",
            "1286/1286 [==============================] - 5s 4ms/step - loss: 0.3603 - accuracy: 0.8644 - val_loss: 0.4680 - val_accuracy: 0.8299\n",
            "Epoch 57/100\n",
            "1286/1286 [==============================] - 4s 3ms/step - loss: 0.3600 - accuracy: 0.8670 - val_loss: 0.4733 - val_accuracy: 0.8332\n",
            "Epoch 58/100\n",
            "  45/1286 [>.............................] - ETA: 2s - loss: 0.3327 - accuracy: 0.8792"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}